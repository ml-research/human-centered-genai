<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description"
          content="Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on a variety of tasks and provide evidence for its versatility and flexibility.">
    <meta property="og:title" content="Semantic Guidance - Project Page"/>
    <meta property="og:description"
          content="Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on a variety of tasks and provide evidence for its versatility and flexibility."/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="/human-centered-genai/static/image/your_banner_image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <meta name="twitter:title" content="Semantic Guidance - Project Page">
    <meta name="twitter:description"
          content="Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on a variety of tasks and provide evidence for its versatility and flexibility.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="/human-centered-genai/static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords"
          content="semantic guidance, image generation, text-to-image, stable diffusion, imagen, dall-e">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Semantic Guidance</title>
    <link rel="icon" type="image/x-icon" href="/human-centered-genai/static/images/favicon.png">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="/human-centered-genai/static/css/bulma.min.css">
    <link rel="stylesheet" href="/human-centered-genai/static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="/human-centered-genai/static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="/human-centered-genai/static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="/human-centered-genai/static/css/index.css">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>


    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="/human-centered-genai/static/js/fontawesome.all.min.js"></script>
    <script src="/human-centered-genai/static/js/bulma-carousel.min.js"></script>
    <script src="/human-centered-genai/static/js/bulma-slider.min.js"></script>
    <script src="/human-centered-genai/static/js/index.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script>hljs.initHighlightingOnLoad();</script>
</head>
<body>
<div style="margin: 10px">
    <a href="/index.html"
       class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fa fa-home"></i>
                        </span>
        <span>Home</span>
    </a>
</div>
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">SEGA: Instructing Diffusion using Semantic Dimensions
                    </h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                  <a href="https://www.aiml.informatik.tu-darmstadt.de/people/mbrack"
                     target="_blank">Manuel Brack</a>,
                        </span>
                        <span class="author-block">
                <a href="https://www.aiml.informatik.tu-darmstadt.de/people/pschramowski" target="_blank">Patrick Schramowski</a>,
                        </span>
                        <span class="author-block">
                    <a href="https://www.aiml.informatik.tu-darmstadt.de/people/ffriedrich" target="_blank">Felix Friedrich</a>,
                  </span>
                        <span class="author-block">
                    <a href="https://www.aiml.informatik.tu-darmstadt.de/people/dhintersdorf" target="_blank">Domink Hintersdorf</a>,
                  </span>
                        <span class="author-block">
                    <a href="https://www.aiml.informatik.tu-darmstadt.de/people/lstruppek" target="_blank">Lukas Struppek</a>,
                  </span>
                        <span class="author-block">
                    <a href="https://www.aiml.informatik.tu-darmstadt.de/people/kkersting" target="_blank">Kristian Kersting</a>
                  </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block">DFKI, hessian.AI, TU Darmstadt, LAION<br>37th Conference on Neural Information Processing Systems (NeurIPS)</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- Arxiv PDF link -->
                            <span class="link-block">
                        <a href="https://arxiv.org/2301.12247.pdf"
                           target="_blank"
                           class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                            <!-- Supplementary PDF link -->
                            <span class="link-block">
                      <a href="https://huggingface.co/spaces/AIML-TUDA/semantic-diffusion"
                         target="_blank"
                         class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-hf"></i>
                      </span>
                      <span>Demo</span>
                    </a>
                  </span>
                            <span class="link-block">
                      <a href="https://huggingface.co/docs/diffusers/api/pipelines/semantic_stable_diffusion"
                         target="_blank"
                         class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-hf"></i>
                      </span>
                      <span>Diffusers</span>
                    </a>
                  </span>

                            <!-- Github link -->
                            <span class="link-block">
                    <a href="https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/semantic_stable_diffusion"
                       target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                            <!-- ArXiv abstract Link -->
                            <span class="link-block">
                  <a href="https://arxiv.org/abs/2301.12247" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-small">
    <div class="hero-body">
        <div class="container has-text-centered">
            <div>
                <img src="/human-centered-genai/static/images/sega/teaser.png" style="max-width: 1200px">
            </div>
        </div>
    </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Text-to-image diffusion models have recently received a lot of interest for their astonishing
                        ability to produce high-fidelity images from text only. However, achieving one-shot generation
                        that aligns with the user's intent is nearly impossible, yet small changes to the input prompt
                        often result in very different images. This leaves the user with little semantic control. To put
                        the user in control, we show how to interact with the diffusion process to flexibly steer it
                        along semantic directions. This semantic guidance (SEGA) allows for subtle and extensive edits,
                        changes in composition and style, as well as optimizing the overall artistic conception. We
                        demonstrate SEGA's effectiveness on a variety of tasks and provide evidence for its versatility
                        and flexibility.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->


<section class="hero is-small">
    <div class="hero-body">
        <div class="container has-text-centered">
            <div>
                <h2 class="title is-3">Methodology</h2>
                <img src="/human-centered-genai/static/images/sega/sega_viz.png" style="max-width: 500px">
                <img src="/human-centered-genai/static/images/sega/king_queen_v2.png" style="max-width: 500px">
            </div>
            <br/>
            <h2 class="has-text-justified">
                The overall idea of SEGA is best explained using a 2D abstraction of the high dimensional &epsilon;-space.
                Intuitively, we can understand the space as a composition of arbitrary sub-spaces representing semantic
                concepts. Let us consider the example of generating an image of a king. The unconditioned noise estimate
                (black dot) starts at some random point in the &epsilon;-space without semantic grounding. The guidance
                corresponding to the prompt ``a portrait of a king'' represents a vector (blue vector) moving us into a
                portion of &epsilon;-space where the concepts `male'
                and royal overlap, resulting in an image of a king.
                We can now further manipulate the generation process using Sega. From the unconditioned starting point,
                we get the directions of `male' and `female' (orange/green lines) using estimates conditioned on the
                respective prompts. If we subtract this inferred `male' direction from our prompt guidance and add the
                `female' one, we now reach a point in the &epsilon;-space at the intersection of the `royal' and
                `female' sub-spaces, i.e., a queen. This vector represents the final direction (red vector) resulting
                from semantic guidance.
            </h2>
            <br/>
            <h2 class="title is-4">Formulation</h2>
            <h2 class="has-text-justified">
                We extend the noise estimate \(\bar\epsilon_\theta\) calculated using classifier-free guidance with a
                dedicated guidance term for editing: \(\bf\gamma\)
                \[\bar\epsilon_\theta(z_t, c_p, c_e) = \epsilon_\theta(z_t) + s_g(\epsilon_\theta(z_t, c_p) -
                \epsilon_\theta(z_t)) + \gamma(z_t, c_e)\]
                where \(z_t\) is the current noisy image, \(c_p\) the encoding of prompt \(p\) and \(c_e\) the encoding
                of our edit instruction \(e\).
                And \(\bar\epsilon_\theta\) is the noise estimate produced by the diffusion model with learned
                parameters \(\theta\).
                For multiple edit instructions \(e_i\) we calculate a dedicated guidance term \(\gamma^i\) with each
                defining their own hyperparamters, allowing for fine-grained semantic control.
            </h2>
        </div>
    </div>
</section>

<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container has-text-centered">
            <div>
                <h2 class="title is-3">Properties of SEGA</h2>
                <div class="columns is-centered">
                    <div class="column">
                        <h2 class="has-text-justified">
                            <b>Robustness.</b>
                            SEGA behaves robustly for incorporating arbitrary concepts into the generated image. Our
                            method
                            performs best-effort integration of the target concept for various image compositions.
                        </h2>
                    </div>
                    <div class="column">
                        <h2 class="has-text-justified">
                            <b>Monotonicity.</b> The magnitude of a semantic concept in an image scales monotonically
                            with the
                            strength of the semantic guidance vector.
                        </h2>
                    </div>
                    <div class="column">
                        <h2 class="has-text-justified">
                            <b>Isolation.</b> Different concepts are largely isolated and thus do not interfere with
                            each other.
                            Isolation enables users to perform multiple changes simultaneously.
                        </h2>
                    </div>
                    <div class="column">
                        <h2 class="has-text-justified">
                            <b>Efficiency &#38; Versatility.</b> SEGA does not require any additional training or tuning
                            and can be plugged in at inference. Our method is also completely architecture-agnostic and
                            usable with any model trained for classifier-free guidance.
                        </h2>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>

<section class="hero is-small">
    <div class="hero-body">
        <div class="container has-text-centered">
            <div>

                <div class="columns is-centered">
                    <div class="column">
                        <h2 class="has-text-justified">
                            <img src="/human-centered-genai/static/images/sega/style.gif" style="max-width: 500px">
                        </h2>
                    </div>
                    <div class="column">
                        <h2 class="has-text-justified">
                            <img src="/human-centered-genai/static/images/sega/car.gif" style="max-width: 500px">
                        </h2>
                    </div>

                </div>
            </div>

        </div>
    </div>
</section>

<!--Image carousel
<section class="hero is-small is-light">
   <div class="hero-body">
       <div class="container">
           <div id="results-carousel" class="carousel results-carousel">
               <div class="item">
                    Your image here
                   <img src="/human-centered-genai/static/images/sega/sega_example_1.png" alt="MY ALT TEXT"/>
                   <h2 class="subtitle has-text-centered">
                       SEGA allows for flexible manipulation of image generation using textual instructions.
                   </h2>
               </div>
               <div class="item">
                    Your image here
                   <img src="/human-centered-genai/static/images/sega/sega_example_2.png" alt="MY ALT TEXT"/>
                   <h2 class="subtitle has-text-centered">
                       Multiple concepts can be combined arbitrarily to achieve complex changes.
                   </h2>
               </div>
               <div class="item">
                    Your image here
                   <img src="/human-centered-genai/static/images/sega/sega_example_3.png" alt="MY ALT TEXT"/>
                   <h2 class="subtitle has-text-centered">
                       SEGA is architecture-agnostic and can be employed for any model using classifier-free
                       guidance.
                   </h2>
               </div>
               <div class="item">
                    Your image here
                   <img src="/human-centered-genai/static/images/sega/sega_example_4.png" alt="MY ALT TEXT"/>
                   <h2 class="subtitle has-text-centered">
                       SEGA is architecture-agnostic and can be employed for any model using classifier-free
                       guidance.
                   </h2>
               </div>
           </div>
       </div>
   </div>
</section>
End image carousel -->

<section class="hero is-small">
    <div class="hero-body">
        <div class="container has-text-centered">
            <!-- Paper video. -->
            <h2 class="title is-3">Implementation and Usage</h2>
            <p>Semantic Guidance if fully integrated into the diffusers library. For more details check out the <a
                    style="color:dodgerblue"
                    href="https://huggingface.co/docs/diffusers/api/pipelines/semantic_stable_diffusion">documentation.</a>
                An exemplary use case could look like this:
            </p>
            <pre><code class="python has-text-justified">import torch
from diffusers import SemanticStableDiffusionPipeline

pipe = SemanticStableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
pipe = pipe.to("cuda")

out = pipe(
    prompt="a photo of the face of a woman",
    num_images_per_prompt=1,
    guidance_scale=7,
    editing_prompt=[
        "smiling, smile",  # Concepts to apply
        "glasses, wearing glasses",
        "curls, wavy hair, curly hair",
        "beard, full beard, mustache",
    ],
    reverse_editing_direction=[False, False, False, False],  # Direction of guidance i.e. increase all concepts
    edit_warmup_steps=[10, 10, 10, 10],  # Warmup period for each concept
    edit_guidance_scale=[4, 5, 5, 5.4],  # Guidance scale for each concept
    edit_threshold=[
        0.99,
        0.975,
        0.925,
        0.96,
    ],  # Threshold for each concept. Threshold equals the percentile of the latent space that will be discarded. I.e. threshold=0.99 uses 1% of the latent dimensions
    edit_momentum_scale=0.3,  # Momentum scale that will be added to the latent guidance
    edit_mom_beta=0.6,  # Momentum beta
    edit_weights=[1, 1, 1, 1, 1],  # Weights of the individual concepts against each other
)
      </code></pre>
            <p>A standalone implementation of the approach for use in further research can be found <a
                    style="color: dodgerblue"
                    href="https://github.com/ml-research/semantic-image-editing">here.</a></p>
            Additionally, we implemented SEGA for multiple other models in <a
                    style="color: dodgerblue"
                    href="https://github.com/ml-research/EFMthis project.">this project.</a>
        </div>
    </div>
</section>


<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <h2 class="title">Poster</h2>

            <iframe src="/human-centered-genai/static/pdfs/neurips_poster_sega.pdf" width="100%" height="600px">
            </iframe>

        </div>
    </div>
</section>


<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{brack2023sega,
      title={SEGA: Instructing Text-to-Image Models using Semantic Guidance},
      author={Manuel Brack and Felix Friedrich and Dominik Hintersdorf and Lukas Struppek and Patrick Schramowski and Kristian Kersting},
      year = {2023},
      booktitle = {Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS)},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This page was built using the <a
                            href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic
                        Project Page Template</a>.
                        You are free to borrow the of this website, we just ask that you link back to this page in the
                        footer. <br> This website is licensed under a <a rel="license"
                                                                         href="http://creativecommons.org/licenses/by-sa/4.0/"
                                                                         target="_blank">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
